{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using HDF5\n",
    "\n",
    "f = h5open(\"$(path)/train.hdf5\")\n",
    "N = length(unique(f[\"label\"][1,:]))\n",
    "close(f)\n",
    "\n",
    "path = \"data\"\n",
    "base_dir = \"snapshots_dropout_fc\"\n",
    "use_gpu = false\n",
    "\n",
    "if use_gpu\n",
    "  ENV[\"MOCHA_USE_CUDA\"] = \"true\"\n",
    "else\n",
    "  ENV[\"MOCHA_USE_NATIVE_EXT\"] = \"true\"\n",
    "  blas_set_num_threads(1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13-Aug 18:56:01:INFO:root:Constructing net SV-train on CPUBackend...\n",
      "13-Aug 18:56:01:INFO:root:Topological sorting 8 layers...\n",
      "13-Aug 18:56:01:INFO:root:Setup layers...\n",
      "13-Aug 18:56:03:INFO:root:Network constructed!\n",
      "************************************************************\n",
      "          NAME: SV-train\n",
      "       BACKEND: CPUBackend\n",
      "  ARCHITECTURE: 8 layers\n",
      "............................................................\n",
      " *** HDF5DataLayer(train-data)\n",
      "    Outputs ---------------------------\n",
      "          data: Blob(20 x 20 x 1 x 1)\n",
      "         label: Blob(1 x 1)\n",
      "............................................................\n",
      " *** DropoutLayer(drop_in)\n",
      "    Inputs ----------------------------\n",
      "          data: Blob(20 x 20 x 1 x 1)\n",
      "............................................................\n",
      " *** InnerProductLayer(fc1)\n",
      "    Inputs ----------------------------\n",
      "          data: Blob(20 x 20 x 1 x 1)\n",
      "    Outputs ---------------------------\n",
      "           fc1: Blob(1200 x 1)\n",
      "............................................................\n",
      " *** DropoutLayer(drop_fc1)\n",
      "    Inputs ----------------------------\n",
      "           fc1: Blob(1200 x 1)\n",
      "............................................................\n",
      " *** InnerProductLayer(fc2)\n",
      "    Inputs ----------------------------\n",
      "           fc1: Blob(1200 x 1)\n",
      "    Outputs ---------------------------\n",
      "           fc2: Blob(1200 x 1)\n",
      "............................................................\n",
      " *** DropoutLayer(drop_fc2)\n",
      "    Inputs ----------------------------\n",
      "           fc2: Blob(1200 x 1)\n",
      "............................................................\n",
      " *** InnerProductLayer(out)\n",
      "    Inputs ----------------------------\n",
      "           fc2: Blob(1200 x 1)\n",
      "    Outputs ---------------------------\n",
      "           out: Blob(62 x 1)\n",
      "............................................................\n",
      " *** SoftmaxLossLayer(loss)\n",
      "    Inputs ----------------------------\n",
      "           out: Blob(62 x 1)\n",
      "         label: Blob(1 x 1)\n",
      "************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "using Mocha\n",
    "srand(333)\n",
    "\n",
    "data_layer  = HDF5DataLayer(name=\"train-data\", source=\"$(path)/train.txt\", batch_size=1)\n",
    "# each fully connected layer uses a ReLU activation and a constraint on the L2 norm of the weights\n",
    "fc1_layer   = InnerProductLayer(name=\"fc1\", output_dim=1200, neuron=Neurons.ReLU(),\n",
    "                                weight_init = GaussianInitializer(std=0.01),\n",
    "                                bottoms=[:data], tops=[:fc1])\n",
    "fc2_layer   = InnerProductLayer(name=\"fc2\", output_dim=1200, neuron=Neurons.ReLU(),\n",
    "                                weight_init = GaussianInitializer(std=0.01),\n",
    "                                weight_cons = L2Cons(4.5),\n",
    "                                bottoms=[:fc1], tops=[:fc2])\n",
    "fc3_layer   = InnerProductLayer(name=\"out\", output_dim=N, bottoms=[:fc2],\n",
    "                                weight_init = ConstantInitializer(0),\n",
    "                                weight_cons = L2Cons(4.5),\n",
    "                                tops=[:out])\n",
    "loss_layer  = SoftmaxLossLayer(name=\"loss\", bottoms=[:out,:label])\n",
    "\n",
    "# setup dropout for the different layers\n",
    "# we use 20% dropout on the inputs and 50% dropout in the hidden layers\n",
    "# as these values were previously found to be good defaults\n",
    "drop_input  = DropoutLayer(name=\"drop_in\", bottoms=[:data], ratio=0.2)\n",
    "drop_fc1 = DropoutLayer(name=\"drop_fc1\", bottoms=[:fc1], ratio=0.5)\n",
    "drop_fc2  = DropoutLayer(name=\"drop_fc2\", bottoms=[:fc2], ratio=0.5)\n",
    "\n",
    "backend = use_gpu ? GPUBackend() : CPUBackend()\n",
    "init(backend)\n",
    "\n",
    "common_layers = [fc1_layer, fc2_layer, fc3_layer]\n",
    "drop_layers = [drop_input, drop_fc1, drop_fc2]\n",
    "# put training net together, note that the correct ordering will automatically be established by the constructor\n",
    "net = Net(\"SV-train\", backend, [data_layer, common_layers..., drop_layers..., loss_layer])\n",
    "println(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{CoffeeBreak,1}:\n",
       " CoffeeBreak(TrainingSummary(true,true,false,false),100,0)\n",
       " CoffeeBreak(Snapshot(\"snapshots_dropout_fc\"),5000,0)     "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we let the learning rate decrease by 0.998 in each epoch (=600 batches of size 100)\n",
    "# and let the momentum increase linearly from 0.5 to 0.9 over 500 epochs\n",
    "# which is equivalent to an increase step of 0.0008\n",
    "# training is done for 2000 epochs\n",
    "params = SolverParameters(max_iter=60*2000, regu_coef=0.00,\n",
    "                          mom_policy=MomPolicy.Linear(0.5, 0.0008, 100, 0.9),\n",
    "                          lr_policy=LRPolicy.Step(0.1, 0.998, 100),\n",
    "                          load_from=base_dir)\n",
    "solver = SGD(params)\n",
    "\n",
    "setup_coffee_lounge(solver, save_into=\"$(base_dir)/statistics.jld\", every_n_iter=5000)\n",
    "\n",
    "# report training progress every 100 iterations\n",
    "add_coffee_break(solver, TrainingSummary(), every_n_iter=100)\n",
    "\n",
    "# save snapshots every 5000 iterations\n",
    "add_coffee_break(solver, Snapshot(base_dir), every_n_iter=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure accuracy check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13-Aug 18:56:09:INFO:root:Constructing net SV-test on CPUBackend...\n",
      "13-Aug 18:56:09:INFO:root:Topological sorting 5 layers...\n",
      "13-Aug 18:56:09:INFO:root:Setup layers...\n",
      "13-Aug 18:56:09:DEBUG:root:InnerProductLayer(fc1): sharing weights and bias\n",
      "13-Aug 18:56:09:DEBUG:root:InnerProductLayer(fc2): sharing weights and bias\n",
      "13-Aug 18:56:09:DEBUG:root:InnerProductLayer(out): sharing weights and bias\n",
      "13-Aug 18:56:09:INFO:root:Network constructed!\n",
      "************************************************************\n",
      "          NAME: SV-test\n",
      "       BACKEND: CPUBackend\n",
      "  ARCHITECTURE: 5 layers\n",
      "............................................................\n",
      " *** HDF5DataLayer(validation-data)\n",
      "    Outputs ---------------------------\n",
      "          data: Blob(20 x 20 x 1 x 1)\n",
      "         label: Blob(1 x 1)\n",
      "............................................................\n",
      " *** InnerProductLayer(fc1)\n",
      "    Inputs ----------------------------\n",
      "          data: Blob(20 x 20 x 1 x 1)\n",
      "    Outputs ---------------------------\n",
      "           fc1: Blob(1200 x 1)\n",
      "............................................................\n",
      " *** InnerProductLayer(fc2)\n",
      "    Inputs ----------------------------\n",
      "           fc1: Blob(1200 x 1)\n",
      "    Outputs ---------------------------\n",
      "           fc2: Blob(1200 x 1)\n",
      "............................................................\n",
      " *** InnerProductLayer(out)\n",
      "    Inputs ----------------------------\n",
      "           fc2: Blob(1200 x 1)\n",
      "    Outputs ---------------------------\n",
      "           out: Blob(62 x 1)\n",
      "............................................................\n",
      " *** AccuracyLayer(validation-accuracy)\n",
      "    Inputs ----------------------------\n",
      "           out: Blob(62 x 1)\n",
      "         label: Blob(1 x 1)\n",
      "************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show performance on test data every 600 iterations (one epoch)\n",
    "data_layer_valid = HDF5DataLayer(name=\"validation-data\", source=\"$(path)/validation.txt\", batch_size=1)\n",
    "acc_layer = AccuracyLayer(name=\"validation-accuracy\", bottoms=[:out, :label], report_error=true)\n",
    "valid_net = Net(\"SV-test\", backend, [data_layer_valid, common_layers..., acc_layer])\n",
    "add_coffee_break(solver, ValidationPerformance(valid_net), every_n_iter=600)\n",
    "\n",
    "println(valid_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13-Aug 18:56:15:DEBUG:root:Checking network topology for back-propagation\n",
      "13-Aug 18:56:15:DEBUG:root:Init network SV-train\n",
      "13-Aug 18:56:15:DEBUG:root:Init parameter weight for layer fc1\n",
      "13-Aug 18:56:15:DEBUG:root:Init parameter bias for layer fc1\n",
      "13-Aug 18:56:15:DEBUG:root:Init parameter weight for layer fc2\n",
      "13-Aug 18:56:15:DEBUG:root:Init parameter bias for layer fc2\n",
      "13-Aug 18:56:15:DEBUG:root:Init parameter weight for layer out\n",
      "13-Aug 18:56:15:DEBUG:root:Init parameter bias for layer out\n",
      "13-Aug 18:56:16:DEBUG:root:Initializing coffee breaks\n",
      "13-Aug 18:56:16:INFO:root:Snapshot directory snapshots_dropout_fc already exists\n",
      "13-Aug 18:56:16:DEBUG:root:Init network SV-test\n",
      "13-Aug 18:56:17:INFO:root:ITER = 000000:: TRAIN obj-val = 4.12713432\n",
      "13-Aug 18:56:17:INFO:root:Saving snapshot to snapshot-000000.jld...\n",
      "13-Aug 18:56:17:DEBUG:root:Saving parameters for layer fc1\n",
      "13-Aug 18:56:17:DEBUG:root:Saving parameters for layer fc2\n",
      "13-Aug 18:56:17:DEBUG:root:Saving parameters for layer out\n",
      "13-Aug 18:56:25:INFO:root:\n",
      "13-Aug 18:56:25:INFO:root:## Performance on Validation Set after 0 iterations\n",
      "13-Aug 18:56:25:INFO:root:---------------------------------------------------------\n",
      "13-Aug 18:56:25:INFO:root:  Accuracy (avg over 1570) = 0.0000%\n",
      "13-Aug 18:56:25:INFO:root:---------------------------------------------------------\n",
      "13-Aug 18:56:25:INFO:root:\n",
      "13-Aug 18:56:27:DEBUG:root:Entering solver loop\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "BoundsError()\nwhile loading In[13], in expression starting on line 1",
     "output_type": "error",
     "traceback": [
      "BoundsError()\nwhile loading In[13], in expression starting on line 1",
      "",
      " in checkbounds at abstractarray.jl:62",
      " in checkbounds at abstractarray.jl:70",
      " in broadcast_getindex! at broadcast.jl:244",
      " in broadcast_getindex at broadcast.jl:241",
      " in forward at /Users/quetzal/.julia/v0.3/Mocha/src/layers/multinomial-logistic-loss.jl:102",
      " in forward at /Users/quetzal/.julia/v0.3/Mocha/src/layers/softmax-loss.jl:46",
      " in forward at /Users/quetzal/.julia/v0.3/Mocha/src/net.jl:148",
      " in solve at /Users/quetzal/.julia/v0.3/Mocha/src/solvers.jl:355"
     ]
    }
   ],
   "source": [
    "solve(solver, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13-Aug 18:15:05:DEBUG:root:Destroying network SV-train\n",
      "13-Aug 18:15:05:DEBUG:root:Destroying network SV-test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{String,Array{AbstractParameter,1}} with 0 entries"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destroy(net)\n",
    "destroy(valid_net)\n",
    "shutdown(backend)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.10",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
